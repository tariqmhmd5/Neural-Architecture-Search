{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b9dfcb1",
   "metadata": {},
   "source": [
    "# Assignment 5: Neuroevolution: Neural Architecture Search (NAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "306d1291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4ac5582",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Digits(Dataset):\n",
    "    \"\"\"Scikit-Learn Digits dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, mode='train', transforms=None):\n",
    "        digits = load_digits()\n",
    "        if mode == 'train':\n",
    "            self.data = digits.data[:1000].astype(np.float32)\n",
    "            self.targets = digits.target[:1000]\n",
    "        elif mode == 'val':\n",
    "            self.data = digits.data[1000:1350].astype(np.float32)\n",
    "            self.targets = digits.target[1000:1350]\n",
    "        else:\n",
    "            self.data = digits.data[1350:].astype(np.float32)\n",
    "            self.targets = digits.target[1350:]\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_x = self.data[idx]\n",
    "        sample_y = self.targets[idx]\n",
    "        if self.transforms:\n",
    "            sample_x = self.transforms(sample_x)\n",
    "        return (sample_x, sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44abc428",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.size = size # a list\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == np.prod(self.size)\n",
    "        return x.view(x.shape[0], *self.size)\n",
    "\n",
    "# This module flattens an input (tensor -> matrix) by blending dimensions \n",
    "# beyond the batch size.\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "  \n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90a16c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierNeuralNet(nn.Module):\n",
    "    def __init__(self, classnet):\n",
    "        super(ClassifierNeuralNet, self).__init__()\n",
    "        # We provide a sequential module with layers and activations\n",
    "        self.classnet = classnet\n",
    "        # The loss function (the negative log-likelihood)\n",
    "        self.nll = nn.NLLLoss(reduction='none') #it requires log-softmax as input!!\n",
    "\n",
    "    # This function classifies an image x to a class.\n",
    "    # The output must be a class label (long).\n",
    "    def classify(self, x):\n",
    "\n",
    "        y_pred = self.classnet(x)\n",
    "        _,pred_label = torch.max(y_pred, dim = 1)\n",
    "\n",
    "        return pred_label\n",
    "\n",
    "    # This function is crucial for a module in PyTorch.\n",
    "    # In our framework, this class outputs a value of the loss function.\n",
    "    def forward(self, x, y, reduction='avg'):\n",
    "\n",
    "        loss = self.nll(self.classnet(x),y.long())\n",
    "\n",
    "        if reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e80dcf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(test_loader, model_best=None, epoch=None):\n",
    "  \n",
    "    model_best.eval()# set the model to the evaluation mode\n",
    "    loss_test = 0.\n",
    "    loss_error = 0.\n",
    "    N = 0.\n",
    "    # start evaluation\n",
    "    for indx_batch, (test_batch, test_targets) in enumerate(test_loader):\n",
    "        # loss (nll)\n",
    "        loss_test_batch = model_best.forward(test_batch, test_targets, reduction='sum')\n",
    "        loss_test = loss_test + loss_test_batch.item()\n",
    "        # classification error\n",
    "        y_pred = model_best.classify(test_batch)\n",
    "        e = 1.*(y_pred == test_targets)\n",
    "        loss_error = loss_error + (1. - e).sum().item()\n",
    "        # the number of examples\n",
    "        N = N + test_batch.shape[0]\n",
    "    # divide by the number of examples\n",
    "    loss_test = loss_test / N\n",
    "    loss_error = loss_error / N\n",
    "\n",
    "    # Print the performance\n",
    "    if epoch is None:\n",
    "        print(f'-> FINAL PERFORMANCE: nll={loss_test}, ce={loss_error}')\n",
    "    else:\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch: {epoch}, val nll={loss_test}, val ce={loss_error}')\n",
    "\n",
    "    return loss_test, loss_error\n",
    "\n",
    "def training(max_patience, num_epochs, model, optimizer, training_loader, val_loader):\n",
    "    nll_val = []\n",
    "    error_val = []\n",
    "    best_nll = 1000.\n",
    "    patience = 0\n",
    "\n",
    "    # Main training loop\n",
    "    for e in range(num_epochs):\n",
    "        model.train() # set the model to the training mode\n",
    "        # load batches\n",
    "        for indx_batch, (batch, targets) in enumerate(training_loader):\n",
    "          # calculate the forward pass (loss function for given images and labels)\n",
    "          loss = model.forward(batch, targets)\n",
    "          # remember we need to zero gradients! Just in case!\n",
    "          optimizer.zero_grad()\n",
    "          # calculate backward pass\n",
    "          loss.backward(retain_graph=True)\n",
    "          # run the optimizer\n",
    "          optimizer.step()\n",
    "\n",
    "        # Validation: Evaluate the model on the validation data\n",
    "        loss_e, error_e = evaluation(val_loader, model_best=model, epoch=e)\n",
    "        nll_val.append(loss_e)  # save for plotting\n",
    "        error_val.append(error_e)  # save for plotting\n",
    "\n",
    "        # Early-stopping: update the best performing model and break training if no \n",
    "        # progress is observed.\n",
    "        if e == 0:\n",
    "            pass\n",
    "        else:\n",
    "            if loss_e < best_nll:\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience = patience + 1\n",
    "\n",
    "        if patience > max_patience:\n",
    "            break\n",
    "\n",
    "    # Return nll and classification error.\n",
    "    nll_val = np.asarray(nll_val)\n",
    "    error_val = np.asarray(error_val)\n",
    "    return nll_val, error_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13d4eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAS():\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "    \n",
    "    def network(self):\n",
    "        fltr = random.choice(self.params['fltr'])\n",
    "        ker_size = random.choice(self.params['ker_size'])\n",
    "        pad = 1 if ker_size == 3 else 2\n",
    "        pooling = random.choice(self.params['pooling'])\n",
    "        activation = random.choice(self.params['activation'])\n",
    "        l_neuron = random.randrange(10,110,10)\n",
    "\n",
    "        for_size = nn.Sequential(Reshape((1,8,8)),\n",
    "                             nn.Conv2d(1,fltr,ker_size,1,pad),\n",
    "                             activation,\n",
    "                             pooling,\n",
    "                             Flatten())\n",
    "        flat_size = for_size(torch.randn(1,64)).shape[1]\n",
    "\n",
    "\n",
    "        classnet = nn.Sequential(Reshape((1,8,8)),\n",
    "                             nn.Conv2d(1,fltr,ker_size,1,pad),\n",
    "                             activation,\n",
    "                             pooling,\n",
    "                             Flatten(),\n",
    "                             nn.Linear(flat_size,l_neuron),\n",
    "                             activation,\n",
    "                             nn.Linear(l_neuron,10),\n",
    "                             nn.LogSoftmax(dim=1))\n",
    "        return classnet\n",
    "\n",
    "    def create_population(self,no_of_childs):\n",
    "        networks = []\n",
    "        for i in range(no_of_childs):\n",
    "            networks.append(self.network())\n",
    "        return networks\n",
    "\n",
    "    def count_parameters(self,model):\n",
    "        total_params = 0\n",
    "        for name, parameter in model.named_parameters():\n",
    "            if not parameter.requires_grad: continue\n",
    "            param = parameter.numel()\n",
    "            total_params+=param\n",
    "        return total_params\n",
    "\n",
    "    def objective(self,population,class_errors):\n",
    "        weights = []\n",
    "        for child in population:\n",
    "            weights.append(self.count_parameters(child))\n",
    "        max_weight = max(weights)\n",
    "        objs = {}\n",
    "        for i in range(len(weights)):\n",
    "            obj = (class_errors[i]) + 0.01 * (weights[i]/max_weight)\n",
    "            objs[obj] = i\n",
    "        return objs\n",
    "    \n",
    "    def find_best_model(self,population,class_errors):\n",
    "        best_model_index = sorted(self.objective(population,class_errors).items())[0][1]\n",
    "        best_model_objective = sorted(self.objective(population,class_errors).items())[0][0]\n",
    "        print(f'\\n Best Model with objecive {best_model_objective:.4f}: \\n {population[best_model_index]}')\n",
    "\n",
    "    def train(self,population,num_epochs,training_loader,val_loader,test_loader,lr,wd,max_patience):\n",
    "        pop = population\n",
    "        class_errors = []\n",
    "        for i,child in enumerate(pop):\n",
    "            print('Training Child: ',i)\n",
    "            model = ClassifierNeuralNet(child)\n",
    "            optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=lr, weight_decay=wd) \n",
    "            nll_val, error_val = training(max_patience=max_patience,\n",
    "                                    num_epochs=num_epochs,\n",
    "                                    model=model,\n",
    "                                    optimizer=optimizer,\n",
    "                                    training_loader=training_loader,\n",
    "                                    val_loader=val_loader)\n",
    "            test_loss, test_error = evaluation(test_loader=test_loader,model_best=model)\n",
    "            class_errors.append(test_error)\n",
    "        return class_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5244f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training, validation and test sets.\n",
    "train_data = Digits(mode='train')\n",
    "val_data = Digits(mode='val')\n",
    "test_data = Digits(mode='test')\n",
    "\n",
    "# Initialize data loaders.\n",
    "training_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67656b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Child:  0\n",
      "Epoch: 0, val nll=0.3875938960484096, val ce=0.08571428571428572\n",
      "Epoch: 10, val nll=0.10159126520156861, val ce=0.02857142857142857\n",
      "-> FINAL PERFORMANCE: nll=0.2944671366305426, ce=0.07158836689038031\n",
      "Training Child:  1\n",
      "Epoch: 0, val nll=0.8345775822230748, val ce=0.1\n",
      "Epoch: 10, val nll=0.16879947389875138, val ce=0.03428571428571429\n",
      "-> FINAL PERFORMANCE: nll=0.2937076811822469, ce=0.07606263982102908\n",
      "Training Child:  2\n",
      "Epoch: 0, val nll=0.7155780247279576, val ce=0.11142857142857143\n",
      "Epoch: 10, val nll=0.11437022890363421, val ce=0.022857142857142857\n",
      "-> FINAL PERFORMANCE: nll=0.2814940243492724, ce=0.0894854586129754\n",
      "Training Child:  3\n",
      "Epoch: 0, val nll=1.5503225490025112, val ce=0.3142857142857143\n",
      "Epoch: 10, val nll=0.18459584508623395, val ce=0.05142857142857143\n",
      "-> FINAL PERFORMANCE: nll=0.39454610129064094, ce=0.12080536912751678\n",
      "Training Child:  4\n",
      "Epoch: 0, val nll=0.9477011162894112, val ce=0.10857142857142857\n",
      "Epoch: 10, val nll=0.1252859262057713, val ce=0.025714285714285714\n",
      "-> FINAL PERFORMANCE: nll=0.2818837496524956, ce=0.08501118568232663\n",
      "\n",
      " Best Model with objecive 0.0816: \n",
      " Sequential(\n",
      "  (0): Reshape()\n",
      "  (1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (2): ReLU()\n",
      "  (3): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (4): Flatten()\n",
      "  (5): Linear(in_features=2048, out_features=70, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): Linear(in_features=70, out_features=10, bias=True)\n",
      "  (8): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if __name__ =='__main__':\n",
    "    # -> training hyperparams\n",
    "    lr = 1e-3 # learning rate\n",
    "    wd = 1e-5 # weight decay\n",
    "    num_epochs = 11 # max. number of epochs\n",
    "    max_patience = 100 # an early stopping is used, if training doesn't improve for longer than 20 epochs, it is stopped\n",
    "    no_of_childs = 5  #no. of child networks in population \n",
    "    params = {\n",
    "        'fltr':[8,16,32],\n",
    "        'ker_size':[3,5],\n",
    "        'activation':[nn.ReLU(),nn.Sigmoid(),nn.Tanh(),nn.Softplus(),nn.ELU()],\n",
    "        'pooling':[nn.MaxPool2d(2),nn.MaxPool2d(1),nn.AvgPool2d(2),nn.AvgPool2d(1)],\n",
    "    }\n",
    "    \n",
    "    networks = NAS(params)\n",
    "    population = networks.create_population(no_of_childs)\n",
    "    class_errors =  networks.train(population=population,\n",
    "                                   num_epochs=num_epochs,\n",
    "                                   training_loader=training_loader,\n",
    "                                   val_loader=val_loader,\n",
    "                                   test_loader=test_loader,lr=lr,wd=wd,max_patience=max_patience)\n",
    "    \n",
    "    networks.find_best_model(population,class_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfbb9e8",
   "metadata": {},
   "source": [
    "Short Summary:\n",
    "\n",
    "1) it randomly generate the (numbers_of_childs) networks for the given random parameters.\n",
    "\n",
    "2) then it train each network on train set and eval in test set\n",
    "\n",
    "3) then it calculates the objective for each network\n",
    "\n",
    "4) based on objective function it finds the best model  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c11ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
